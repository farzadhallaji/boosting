{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c752068a-6f99-4b23-9491-197fca4fc385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifierMe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26100092-1cd3-4e45-afec-214c2353de13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc857511-777b-4448-a929-a1089c3cb42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifierMe\n",
    "class OUBoost(AdaBoostClassifierMe):\n",
    "    \"\"\"Implementation of RUSBoost.\n",
    "    RUSBoost introduces data sampling into the AdaBoost algorithm by\n",
    "    undersampling the majority class using random undersampling (with or\n",
    "    without replacement) on each boosting iteration [1].\n",
    "    This implementation inherits methods from the scikit-learn \n",
    "    AdaBoostClassifier class, only modifying the `fit` method.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int, optional (default=100)\n",
    "        Number of new synthetic samples per boosting step.\n",
    "    min_ratio : float (default=1.0)\n",
    "        Minimum ratio of majority to minority class samples to generate.\n",
    "    with_replacement : bool, optional (default=True)\n",
    "        Undersample with replacement.\n",
    "    base_estimator : object, optional (default=DecisionTreeClassifier)\n",
    "        The base estimator from which the boosted ensemble is built.\n",
    "        Support for sample weighting is required, as well as proper `classes_`\n",
    "        and `n_classes_` attributes.\n",
    "    n_estimators : int, optional (default=50)\n",
    "        The maximum number of estimators at which boosting is terminated.\n",
    "        In case of perfect fit, the learning procedure is stopped early.\n",
    "    learning_rate : float, optional (default=1.)\n",
    "        Learning rate shrinks the contribution of each classifier by\n",
    "        ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
    "        ``n_estimators``.\n",
    "    algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')\n",
    "        If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n",
    "        ``base_estimator`` must support calculation of class probabilities.\n",
    "        If 'SAMME' then use the SAMME discrete boosting algorithm.\n",
    "        The SAMME.R algorithm typically converges faster than SAMME,\n",
    "        achieving a lower test error with fewer boosting iterations.\n",
    "    random_state : int or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator.\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by np.random.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] C. Seiffert, T. M. Khoshgoftaar, J. V. Hulse, and A. Napolitano.\n",
    "           \"RUSBoost: Improving Classification Performance when Training Data\n",
    "           is Skewed\". International Conference on Pattern Recognition\n",
    "           (ICPR), 2008.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_samples=100,\n",
    "                 min_ratio=1.0,\n",
    "                 with_replacement=False,\n",
    "                 base_estimator=None,\n",
    "                 n_estimators=50,\n",
    "                 learning_rate=1.,\n",
    "                 algorithm='SAMME.R',\n",
    "                 random_state=None):\n",
    "\n",
    "        self.n_samples = n_samples\n",
    "        self.min_ratio = min_ratio\n",
    "        self.algorithm = algorithm\n",
    "        self.ou = Sampler(with_replacement=with_replacement,\n",
    "                                      return_indices=True,\n",
    "                                      random_state=random_state)\n",
    "\n",
    "        super(OUBoost, self).__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            random_state=random_state)\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None, minority_target=None):\n",
    "        \"\"\"Build a boosted classifier/regressor from the training set (X, y),\n",
    "        performing random undersampling during each boosting step.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
    "            DOK, or LIL. COO, DOK, and LIL are converted to CSR. The dtype is\n",
    "            forced to DTYPE from tree._tree if the base classifier of this\n",
    "            ensemble weighted boosting classifier is a tree or forest.\n",
    "        y : array-like of shape = [n_samples]\n",
    "            The target values (class labels in classification, real numbers in\n",
    "            regression).\n",
    "        sample_weight : array-like of shape = [n_samples], optional\n",
    "            Sample weights. If None, the sample weights are initialized to\n",
    "            1 / n_samples.\n",
    "        minority_target : int\n",
    "            Minority class label.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        Notes\n",
    "        -----\n",
    "        Based on the scikit-learn v0.18 AdaBoostClassifier and\n",
    "        BaseWeightBoosting `fit` methods.\n",
    "        \"\"\"\n",
    "        # Check that algorithm is supported.\n",
    "        if self.algorithm not in ('SAMME', 'SAMME.R'):\n",
    "            raise ValueError(\"algorithm %s is not supported\" % self.algorithm)\n",
    "\n",
    "        # Check parameters.\n",
    "        if self.learning_rate <= 0:\n",
    "            raise ValueError(\"learning_rate must be greater than zero\")\n",
    "\n",
    "        if (self.base_estimator is None or\n",
    "                isinstance(self.base_estimator, (BaseDecisionTree,\n",
    "                                                 BaseForest))):\n",
    "            DTYPE = np.float64  # from fast_dict.pxd\n",
    "            dtype = DTYPE\n",
    "            accept_sparse = 'csc'\n",
    "        else:\n",
    "            dtype = None\n",
    "            accept_sparse = ['csr', 'csc']\n",
    "\n",
    "        X, y = check_X_y(X, y, accept_sparse=accept_sparse, dtype=dtype,\n",
    "                         y_numeric=is_regressor(self))\n",
    "\n",
    "        if sample_weight is None:\n",
    "            # Initialize weights to 1 / n_samples.\n",
    "            sample_weight = np.empty(X.shape[0], dtype=np.float64)\n",
    "            sample_weight[:] = 1. / X.shape[0]\n",
    "        else:\n",
    "            sample_weight = check_array(sample_weight, ensure_2d=False)\n",
    "            # Normalize existing weights.\n",
    "            sample_weight = sample_weight / sample_weight.sum(dtype=np.float64)\n",
    "\n",
    "            # Check that the sample weights sum is positive.\n",
    "            if sample_weight.sum() <= 0:\n",
    "                raise ValueError(\n",
    "                    \"Attempting to fit with a non-positive \"\n",
    "                    \"weighted number of samples.\")\n",
    "\n",
    "        if minority_target is None:\n",
    "            # Determine the minority class label.\n",
    "            X_org = X\n",
    "            y_org = y\n",
    "            sample_weight_org = sample_weight\n",
    "            \n",
    "            stats_c_ = Counter(y)\n",
    "            maj_c_ = max(stats_c_, key=stats_c_.get)\n",
    "            min_c_ = min(stats_c_, key=stats_c_.get)\n",
    "            self.minority_target = min_c_\n",
    "        else:\n",
    "            self.minority_target = minority_target\n",
    "\n",
    "        self._validate_estimator()\n",
    "\n",
    "\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float64)\n",
    "        self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float64)\n",
    "\n",
    "        random_state = check_random_state(self.random_state)\n",
    "        OX_min = X_org[np.where(y_org == self.minority_target)]\n",
    "\n",
    "\n",
    "        for iboost in range(self.n_estimators):\n",
    "\n",
    "            # Random undersampling step.\n",
    "            X_maj = X_org[np.where(y_org != self.minority_target)]\n",
    "            X_min = X_org[np.where(y_org == self.minority_target)]\n",
    "            \n",
    "            stats_ = Counter(y_org == 1)\n",
    "\n",
    "            ratio=np.sum(y_org)/ y_org.shape[0]\n",
    "     \n",
    "            if  ratio <0.50:\n",
    "                self.ou.fitMin(X_min)\n",
    "                X_syn = self.ou.sampleMin(self.n_samples)\n",
    "                y_syn = np.full(X_syn.shape[0], fill_value=self.minority_target,\n",
    "                              dtype=np.int64)\n",
    "                # Normalize synthetic sample weights based on current training set.\n",
    "                sample_weight_syn = np.empty(X_syn.shape[0], dtype=np.float64)\n",
    "                sample_weight_syn[:] = 1. / (X_org.shape[0])\n",
    "\n",
    "                X_org = np.vstack((X_org, X_syn))\n",
    "                y_org = np.append(y_org, y_syn)\n",
    "\n",
    "                # Combine the weights.\n",
    "                sample_weight_org = \\\n",
    "                 np.append(sample_weight_org, sample_weight_syn).reshape(-1, 1)\n",
    "                sample_weight_org = \\\n",
    "                 np.squeeze(normalize(sample_weight_org, axis=0, norm='l1'))\n",
    "                self.ou.fitMaj(X_org,y_org)\n",
    "                indexs = self.ou.sampleMaj(self.n_samples)\n",
    "                X_maj = X_org[np.where(y_org != self.minority_target)]\n",
    "                y_maj = y_org[np.where(y_org != self.minority_target)]\n",
    "                w_maj = sample_weight_org[np.where(y_org != self.minority_target)]\n",
    "                #  X_rus = X_maj[np.where(y_maj != self.minority_target)][indexs]\n",
    "                X_rus = np.copy(X_maj)[np.where(y_maj != self.minority_target)][indexs]\n",
    "                X_min = X_org[np.where(y_org == self.minority_target)]\n",
    "                #  y_rus = y_maj[np.where(y_maj != self.minority_target)][indexs]\n",
    "                y_rus = np.copy(y_maj)[np.where(y_maj != self.minority_target)][indexs]\n",
    "                y_min = y_org[np.where(y_org == self.minority_target)]\n",
    "                sample_weight_rus = np.copy(w_maj)[np.where(y_maj != self.minority_target)][indexs]\n",
    "                sample_weight_min = sample_weight_org[np.where(y_org == self.minority_target)]\n",
    "                X = np.vstack((X_rus, X_min))\n",
    "                y = np.append(y_rus, y_min)\n",
    "                # Combine the weights.\n",
    "                sample_weight = \\\n",
    "                  np.append(sample_weight_rus, sample_weight_min).reshape(-1, 1)\n",
    "                sample_weight = \\\n",
    "                  np.squeeze(normalize(sample_weight, axis=0, norm='l1'))\n",
    "\n",
    "                \n",
    "            # Boosting step.\n",
    "            sample_weight, estimator_weight, estimator_error,sample_weight_org = self._boost(\n",
    "                iboost,\n",
    "                X, y,\n",
    "                sample_weight,\n",
    "                random_state,X_org,y_org,sample_weight_org)\n",
    "             \n",
    "          \n",
    "            X = X_org\n",
    "            y = y_org\n",
    "            sample_weight = sample_weight_org\n",
    "            \n",
    "            # Early termination.\n",
    "            if sample_weight_org is None:\n",
    "                break\n",
    "\n",
    "            self.estimator_weights_[iboost] = estimator_weight\n",
    "            self.estimator_errors_[iboost] = estimator_error\n",
    "\n",
    "            # Stop if error is zero.\n",
    "            if estimator_error == 0:\n",
    "                break\n",
    "\n",
    "            sample_weight_sum = np.sum(sample_weight_org)\n",
    "\n",
    "            # Stop if the sum of sample weights has become non-positive.\n",
    "            if sample_weight_sum <= 0:\n",
    "                break\n",
    "\n",
    "            if iboost < self.n_estimators - 1:\n",
    "                # Normalize.\n",
    "                sample_weight_org /= sample_weight_sum\n",
    "\n",
    "\n",
    "               \n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d65d68c-91bc-4378-838b-574c0c1d2f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "classification_ouboost = OUBoost(learning_rate=0.3, n_samples=100, n_estimators=50)\n",
    "classification_ouboost.fit(X_train, Y_train) \n",
    "y_pred_ouboost = classification_ouboost.predict(X_test)\n",
    "end = datetime.now()\n",
    "td = (end - start).total_seconds() * 10**3\n",
    "print(f\"The time of execution of OUBoost is : {td:.03f}ms\")\n",
    "proba_ouboost = classification_ouboost.predict_proba(X_test)\n",
    "auc_ouboost = proba_ouboost[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(Y_test, auc_ouboost)\n",
    "auc_result_ouboost.append(metrics.auc(fpr, tpr))\n",
    "gmean_ouboost.append(geometric_mean_score(Y_test, y_pred_ouboost, average=None))\n",
    "mcc_ouboost.append(matthews_corrcoef(Y_test, y_pred_ouboost))\n",
    "\n",
    "print(\"OUBoost\",accuracy_score(Y_test, y_pred_ouboost))\n",
    "print(confusion_matrix(Y_test, y_pred_ouboost))\n",
    "print(classification_report(Y_test, y_pred_ouboost))\n",
    "score_array_ouboost.append(precision_recall_fscore_support(Y_test, y_pred_ouboost, average=None))\n",
    "result_ouboost.append(accuracy_score(Y_test, y_pred_ouboost))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
