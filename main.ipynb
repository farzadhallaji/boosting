{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f584c0b9-9ad7-4ad3-8e90-568385f96abd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f205d1-e543-4532-a098-2be27173f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda install -c conda-forge imbalanced-learn -y\n",
    "# ! pip install nose\n",
    "# ! pip install imbalanced-ensemble           \n",
    "# ! pip install threadpoolctl\n",
    "# ! pip install imbalanced-ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b312338-1179-4c80-bf40-7a14b9d12047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import random  \n",
    "import pandas as pd \n",
    "import random\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc,f1_score, matthews_corrcoef, precision_score, recall_score\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from imbalanced_ensemble.ensemble import SMOTEBoostClassifier\n",
    "from maatpy.classifiers import AdaCost\n",
    "from imbens.ensemble import SelfPacedEnsembleClassifier\n",
    "\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4db973f-9cab-467e-8684-601152376141",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Init "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e62062f-1c88-4047-b296-0ce70ca39b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'Datasets/'\n",
    "datasets  = os.listdir(base_path) \n",
    "classifiers = {\"RUS\": RUSBoostClassifier(random_state=0, algorithm='SAMME', base_estimator=DecisionTreeClassifier(max_depth=10)),\n",
    "              \"SMOTE\": SMOTEBoostClassifier(estimator=DecisionTreeClassifier(max_depth=10), n_estimators = 100, algorithm='SAMME', random_state=0),\n",
    "              \"Ada1\": AdaCost(base_estimator=DecisionTreeClassifier(max_depth=10), n_estimators=100, algorithm='adac1',random_state=0),\n",
    "              \"AdaCost\": AdaCost(base_estimator=DecisionTreeClassifier(max_depth=10), n_estimators=100, algorithm='adacost',random_state=0),\n",
    "              \"SPE\":SelfPacedEnsembleClassifier(estimator=DecisionTreeClassifier(max_depth=10), n_estimators=100, random_state=0)}\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    if not os.path.isdir('./Results/'+datasets[i]):\n",
    "        os.mkdir('./Results/'+datasets[i])\n",
    "    for classifier in classifiers:\n",
    "        if not os.path.isdir('./Results/'+datasets[i]+'/'+classifier):\n",
    "            os.mkdir('./Results/'+datasets[i]+'/'+classifier)\n",
    "            \n",
    "if not os.path.exists('errors.csv'):            \n",
    "    error_df = pd.DataFrame(columns=['dataset','classifier','error'])\n",
    "    error_df.to_csv('errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f6a92d6-54cc-46a5-929a-3918bc55dabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasify(dataset, classifier):\n",
    "    mcc = []\n",
    "    f1 = []\n",
    "    auc_a = []\n",
    "    gmean = []\n",
    "    times = []\n",
    "    y_preds = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    traFiles = sorted(glob.glob(base_path+dataset+'/*tra.dat'))\n",
    "    tstFiles = sorted(glob.glob(base_path+dataset+'/*tst.dat'))\n",
    "    for traPath, tstPath in zip(traFiles, tstFiles):\n",
    "        \n",
    "        df_train = read_dot_dat_file(traPath)\n",
    "        df_test = read_dot_dat_file(tstPath)\n",
    "\n",
    "        x_train= df_train.iloc[:, :-1]\n",
    "        y_train = df_train.iloc[:, -1]\n",
    "        x_test= df_test.iloc[:, :-1]\n",
    "        y_test = df_test.iloc[:, -1]\n",
    "        \n",
    "        #####\n",
    "        # for some dataset get error Unknown label type: 'unknown'\n",
    "        y_train = y_train.astype('int')\n",
    "        y_test = y_test.astype('int')\n",
    "        \n",
    "        st = time.time()\n",
    "        clf = classifiers[classifier]\n",
    "\n",
    "        clf.fit(x_train, y_train)\n",
    "\n",
    "        y_pred = clf.predict(x_test) \n",
    "        et = time.time()\n",
    "        y_preds.append(y_pred)\n",
    "        # compute error\n",
    "        mcc.append(matthews_corrcoef(y_test, y_pred))\n",
    "        #--------------------------------\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "        auc_a.append(auc(fpr, tpr))\n",
    "        #--------------------------------\n",
    "        f1.append(f1_score(y_test, y_pred))\n",
    "        #--------------------------------\n",
    "        gmean.append(geometric_mean_score(y_test, y_pred, labels=[1, -1]))\n",
    "        \n",
    "        #time of train and test\n",
    "        times.append(et - st)\n",
    "        \n",
    "        precisions.append(precision_score(y_test, y_pred))\n",
    "        recalls.append(precision_score(y_test, y_pred))\n",
    "        \n",
    "    return {\"precision\": precisions, \"recall\": recalls, \"mcc\": mcc, \"auc\": auc_a, \"f1\": f1, \"gmean\": gmean, \"exe_time\": times, \"y_pred\": y_pred}\n",
    "# print(datasets[0])\n",
    "# for dataset in datasets:\n",
    "#     clasify(dataset, 'Ada1')\n",
    "# # clasify('yeast4-5-fold', 'SPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e258c6dc-3f64-4df4-89eb-d348e5e5a7c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c03cd98-c546-455a-881a-77f8fd1598ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUS\n",
      "yeast-1-2-8-9_vs_7-5-fold ecoli-0-3-4_vs_5-5-fold new-thyroid2-5-fold SMOTE\n",
      "yeast-1_vs_7-5-fold led7digit-0-2-4-5-6-7-8-9_vs_1-5-fold ecoli-0-6-7_vs_3-5-5-fold glass-0-4_vs_5-5-fold ecoli-0-2-6-7_vs_3-5-5-fold yeast-0-2-5-6_vs_3-7-8-9-5-fold yeast-0-5-6-7-9_vs_4-5-fold ecoli-0-3-4-6_vs_5-5-fold "
     ]
    }
   ],
   "source": [
    "for classifier in classifiers:\n",
    "    print(classifier)\n",
    "    for dataset in datasets:\n",
    "        if not os.path.exists('./Results/'+dataset+'/'+classifier+'/y_pred.npy'):\n",
    "            print(dataset, end=' ')\n",
    "            try:\n",
    "                tmp_res = clasify(dataset, classifier)\n",
    "                res_to_files(dataset, classifier, tmp_res)\n",
    "\n",
    "            except Exception as e:\n",
    "                error_df = pd.read_csv('errors.csv')\n",
    "                new_error = {'dataset': dataset, 'classifier':classifier, 'error':str(e)}\n",
    "                error_df = error_df.append(new_error, ignore_index=True)\n",
    "                error_df.to_csv('errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4dd3c22d-8e51-4549-b055-0166c8ec909c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yeast-1-2-8-9_vs_7-5-fold RUS\n",
      "shuttle-c2-vs-c4-5-fold SMOTE\n",
      "ecoli-0-1-3-7_vs_2-6-5-fold SMOTE\n",
      "yeast4-5-fold Ada1\n",
      "abalone19-5- Ada1\n",
      "abalone19-5-fold Ada1\n",
      "abalone19-5- AdaCost\n",
      "abalone19-5-fold AdaCost\n",
      "shuttle-c2-vs-c4-5-fold STOMEENC\n",
      "ecoli-0-1-3-7_vs_2-6-5-fold STOMEENC\n"
     ]
    }
   ],
   "source": [
    "troubs = {}\n",
    "clssssss = list(classifiers.keys())\n",
    "# clssssss.append('STOMEENC')\n",
    "for classifier in clssssss:\n",
    "    for dataset in datasets:\n",
    "        files = glob.glob('./Results/'+dataset+'/'+classifier+'/*.npy')\n",
    "        if len(files) != 6:\n",
    "            if dataset in troubs:\n",
    "                troubs[dataset].append(classifier)\n",
    "            else:\n",
    "                troubs[dataset] = [classifier]\n",
    "            print(dataset, classifier)\n",
    "        \n",
    "error_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13706bc8-93d6-4ffd-b512-a55f356675c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e03e6d0-944d-4a40-aa70-ca7c0c922009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex,</th>\n",
       "      <th>Length,</th>\n",
       "      <th>Diameter,</th>\n",
       "      <th>Height,</th>\n",
       "      <th>Whole_weight,</th>\n",
       "      <th>Shucked_weight,</th>\n",
       "      <th>Viscera_weight,</th>\n",
       "      <th>Shell_weight</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3334</th>\n",
       "      <td>2</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.7910</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>0.1815</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3335</th>\n",
       "      <td>0</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.8870</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>0.2390</td>\n",
       "      <td>0.2490</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3336</th>\n",
       "      <td>2</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.9660</td>\n",
       "      <td>0.4390</td>\n",
       "      <td>0.2145</td>\n",
       "      <td>0.2605</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3337</th>\n",
       "      <td>2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.205</td>\n",
       "      <td>1.1760</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.2875</td>\n",
       "      <td>0.3080</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3338</th>\n",
       "      <td>0</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.150</td>\n",
       "      <td>1.0945</td>\n",
       "      <td>0.5310</td>\n",
       "      <td>0.2610</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3339 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sex,  Length,  Diameter,  Height,  Whole_weight,  Shucked_weight,  \\\n",
       "0        2    0.455      0.365    0.095         0.5140           0.2245   \n",
       "1        2    0.350      0.265    0.090         0.2255           0.0995   \n",
       "2        0    0.530      0.420    0.135         0.6770           0.2565   \n",
       "3        2    0.440      0.365    0.125         0.5160           0.2155   \n",
       "4        1    0.330      0.255    0.080         0.2050           0.0895   \n",
       "...    ...      ...        ...      ...            ...              ...   \n",
       "3334     2    0.520      0.385    0.165         0.7910           0.3750   \n",
       "3335     0    0.565      0.450    0.165         0.8870           0.3700   \n",
       "3336     2    0.590      0.440    0.135         0.9660           0.4390   \n",
       "3337     2    0.600      0.475    0.205         1.1760           0.5255   \n",
       "3338     0    0.625      0.485    0.150         1.0945           0.5310   \n",
       "\n",
       "      Viscera_weight,  Shell_weight  Class  \n",
       "0              0.1010        0.1500     -1  \n",
       "1              0.0485        0.0700     -1  \n",
       "2              0.1415        0.2100     -1  \n",
       "3              0.1140        0.1550     -1  \n",
       "4              0.0395        0.0550     -1  \n",
       "...               ...           ...    ...  \n",
       "3334           0.1800        0.1815     -1  \n",
       "3335           0.2390        0.2490     -1  \n",
       "3336           0.2145        0.2605     -1  \n",
       "3337           0.2875        0.3080     -1  \n",
       "3338           0.2610        0.2960     -1  \n",
       "\n",
       "[3339 rows x 9 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "path = 'Datasets/abalone19-5-/abalone19-5-1tra.dat'\n",
    "import re\n",
    "\n",
    "datContent = [i.strip().split() for i in open(path).readlines()]\n",
    "r = re.compile(\"@inputs.*\")\n",
    "_at_data = datContent.index(['@data'])\n",
    "assert datContent[0][0] == '@relation'\n",
    "assert datContent[_at_data-1][0] == '@outputs'\n",
    "assert datContent[_at_data-2][0] == '@inputs'\n",
    "assert len(datContent[_at_data-3][2:]) == 2   # Two Class\n",
    "\n",
    "col_names = datContent[_at_data-2][1:]\n",
    "col_names.append(datContent[_at_data-1][1])\n",
    "\n",
    "df = pd.read_csv(path, skiprows=_at_data+1, names=col_names, sep=r', ', engine='python')\n",
    "# df = pd.read_csv(path, skiprows=_at_data+1, names=col_names, sep=\", \", engine='python')\n",
    "\n",
    "class1 = datContent[_at_data-3][2:][0].replace(\"{\",\"\").replace(\",\",\"\")\n",
    "class2 = datContent[_at_data-3][2:][1].replace(\"}\",\"\").replace(\",\",\"\")\n",
    "\n",
    "df['Class'] = df['Class'].replace({class1: 1, class2: -1})\n",
    "\n",
    "needs_to_convert = []\n",
    "for i in range(1,len(col_names)+4):\n",
    "    if datContent[i][0] == '@attribute' and datContent[i][2] == 'nominal':\n",
    "        needs_to_convert.append(col_names[i-1])\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for need_to_convert in needs_to_convert:\n",
    "    le = LabelEncoder()\n",
    "    label = le.fit_transform(df[need_to_convert])        \n",
    "    df[need_to_convert] = label\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e7c21ce-42ae-4398-8790-6dab0a87c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert categorical variables into numerical\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    " \n",
    "# le = LabelEncoder()\n",
    " \n",
    "# # Using .fit_transform function to fit label\n",
    "# # encoder and return encoded label\n",
    "\n",
    "# base_path = 'Datasets/'\n",
    "# need_to_convert = ['abalone19-5-', 'abalone19-5-fold']\n",
    "# for needed in need_to_convert[1:]:\n",
    "#     for datFile in glob.glob(base_path+needed+'/*.dat'):\n",
    "#         print(datFile)\n",
    "#         df = read_dot_dat_file(datFile)\n",
    "#         label = le.fit_transform(df['Sex,'])        \n",
    "#         df['Sex,'] = label\n",
    "#         # df.to_excel(base_path+needed+'/'+datFile.split('/')[-1].split('.')[0]+'.xlsx')\n",
    "#         break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "138cde2d-9c7d-4e61-b733-cb4894e8dc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = read_dot_dat_file(glob.glob(base_path+datasets[0]+'/*.dat')[0])\n",
    "# df.to_excel('temp.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ec1b654-281a-4d72-ba83-ddafec3c725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_excel('temp.xlsx').iloc[:, 0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d75001-10b2-4a26-ba28-7ac17700e79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yeast-1-2-8-9_vs_7-5-fold RUS\n",
    "# shuttle-c2-vs-c4-5-fold SMOTE\n",
    "# ecoli-0-1-3-7_vs_2-6-5-fold SMOTE\n",
    "# yeast4-5-fold Ada1\n",
    "# abalone19-5- Ada1\n",
    "# abalone19-5-fold Ada1\n",
    "# abalone19-5- AdaCost\n",
    "# abalone19-5-fold AdaCost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d7806-0691-4fc2-85b5-c7f6ece4bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for classifier in classifiers:\n",
    "# #     for dataset in datasets:\n",
    "# #         if not os.path.exists('./Results/'+dataset+'/'+classifier+'/y_pred.npy'):\n",
    "# #             print(classifier,\" => \", dataset, \"=>\", end=' ')\n",
    "# #             # tmp_res = clasify(dataset, classifier)\n",
    "# #             # print(tmp_res['auc'])\n",
    "# #             try:\n",
    "# #                 tmp_res = clasify(dataset, classifier)\n",
    "# #                 res_to_files(dataset, classifier, tmp_res)\n",
    "# #             except:\n",
    "# #                 print(dataset, classifier)\n",
    "\n",
    "# classifier = 'Ada1'\n",
    "# for dataset in datasets:\n",
    "#     if not os.path.exists('./Results/'+dataset+'/'+classifier+'/y_pred.npy'):\n",
    "#         print(classifier,\" => \", dataset, \"=>\", end=' ')\n",
    "#         # try:\n",
    "#         tmp_res = clasify(dataset, classifier)\n",
    "#         res_to_files(dataset, classifier, tmp_res)\n",
    "#         # except:\n",
    "#             # print(dataset, classifier)\n",
    "\n",
    "# # classifier = 'AdaCost'\n",
    "# # dataset = 'abalone19-5-'\n",
    "# # if not os.path.exists('./Results/'+dataset+'/'+classifier+'/y_pred.npy'):\n",
    "# #     print(classifier,\" => \", dataset, \"=>\", end=' ')\n",
    "# #     tmp_res = clasify(dataset, classifier)\n",
    "# #     res_to_files(dataset, classifier, tmp_res)\n",
    "# #     print(tmp_res['auc'])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8790a636-85b8-4635-b096-9ac350156341",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Any New Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf06d335-410e-4614-8d69-9ff825c2696f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## STOMEENC\n",
    "https://github.com/Mimimkh/SMOTE-ENC-code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96f5272e-57a1-4420-87a6-8a6720c1b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.utils import check_array, _safe_indexing, sparsefuncs_fast, check_X_y, check_random_state\n",
    "from numbers import Integral\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "\n",
    "class mSMOTENC():\n",
    "    \n",
    "    def __init__(self, categorical_features):\n",
    "        self.categorical_features = categorical_features\n",
    "        \n",
    "    def chk_neighbors(self, nn_object, additional_neighbor):\n",
    "        if isinstance(nn_object, Integral):\n",
    "            return NearestNeighbors(n_neighbors=nn_object + additional_neighbor)\n",
    "        elif isinstance(nn_object, KNeighborsMixin):\n",
    "            return clone(nn_object)\n",
    "        else:\n",
    "            raise_isinstance_error(nn_name, [int, KNeighborsMixin], nn_object)     \n",
    "    \n",
    "    def generate_samples(self, X, nn_data, nn_num, rows, cols, steps, continuous_features_,):\n",
    "        rng = check_random_state(42)\n",
    "\n",
    "        diffs = nn_data[nn_num[rows, cols]] - X[rows]\n",
    "\n",
    "        if sparse.issparse(X):\n",
    "            sparse_func = type(X).__name__\n",
    "            steps = getattr(sparse, sparse_func)(steps)\n",
    "            X_new = X[rows] + steps.multiply(diffs)\n",
    "        else:\n",
    "            X_new = X[rows] + steps * diffs \n",
    "\n",
    "        X_new = (X_new.tolil() if sparse.issparse(X_new) else X_new)\n",
    "        # convert to dense array since scipy.sparse doesn't handle 3D\n",
    "        nn_data = (nn_data.toarray() if sparse.issparse(nn_data) else nn_data)\n",
    "        all_neighbors = nn_data[nn_num[rows]]\n",
    "\n",
    "        for idx in range(continuous_features_.size, X.shape[1]):\n",
    "            mode = stats.mode(all_neighbors[:, :, idx], axis = 1)[0]\n",
    "            X_new[:, idx] = np.ravel(mode)\n",
    "\n",
    "        return X_new\n",
    "    \n",
    "    def make_samples(self, X, y_dtype, y_type, nn_data, nn_num, n_samples, continuous_features_, step_size=1.0):\n",
    "        random_state = check_random_state(42)\n",
    "        samples_indices = random_state.randint(low=0, high=len(nn_num.flatten()), size=n_samples)    \n",
    "        steps = step_size * random_state.uniform(size=n_samples)[:, np.newaxis]\n",
    "        rows = np.floor_divide(samples_indices, nn_num.shape[1])\n",
    "        cols = np.mod(samples_indices, nn_num.shape[1])\n",
    "\n",
    "        X_new = self.generate_samples(X, nn_data, nn_num, rows, cols, steps, continuous_features_)\n",
    "        y_new = np.full(n_samples, fill_value=y_type, dtype=y_dtype)\n",
    "        \n",
    "        return X_new, y_new\n",
    "    \n",
    "    def cat_corr_pandas(self, X, target_df, target_column, target_value):\n",
    "    # X has categorical columns\n",
    "        categorical_columns = list(X.columns)\n",
    "        X = pd.concat([X, target_df], axis=1)\n",
    "\n",
    "        # filter X for target value\n",
    "        is_target = X.loc[:, target_column] == target_value\n",
    "        X_filtered = X.loc[is_target, :]\n",
    "\n",
    "        X_filtered.drop(target_column, axis=1, inplace=True)\n",
    "\n",
    "        # get columns in X\n",
    "        nrows = len(X)\n",
    "        encoded_dict_list = []\n",
    "        nan_dict = dict({})\n",
    "        c = 0\n",
    "        imb_ratio = len(X_filtered)/len(X)\n",
    "        OE_dict = {}\n",
    "        \n",
    "        for column in categorical_columns:\n",
    "            for level in list(X.loc[:, column].unique()):\n",
    "                \n",
    "                # filter rows where level is present\n",
    "                row_level_filter = X.loc[:, column] == level\n",
    "                rows_in_level = len(X.loc[row_level_filter, :])\n",
    "                \n",
    "                # number of rows in level where target is 1\n",
    "                O = len(X.loc[is_target & row_level_filter, :])\n",
    "                E = rows_in_level * imb_ratio\n",
    "                # Encoded value = chi, i.e. (observed - expected)/expected\n",
    "                ENC = (O - E+0.000000000000000000000000000000000000000000000000000000001) / (E+0.000000000000000000000000000000000000000000000000000000001)\n",
    "                # ENC = (O - E+0.0000000000001) / (E+0.0000000000001)\n",
    "                OE_dict[level] = ENC\n",
    "                \n",
    "            encoded_dict_list.append(OE_dict)\n",
    "\n",
    "            X.loc[:, column] = X[column].map(OE_dict)\n",
    "            # print(f'X.loc[:, {column}]', X.loc[:, column])\n",
    "            # nan_idx_array = np.ravel(np.argwhere(np.isnan(X.loc[:, column])))\n",
    "            nan_idx_array = np.array([0])\n",
    "            if len(nan_idx_array) > 0 :\n",
    "                nan_dict[c] = nan_idx_array\n",
    "            c = c + 1\n",
    "            X.loc[:, column].fillna(-1, inplace = True)\n",
    "                \n",
    "        X.drop(target_column, axis=1, inplace=True)\n",
    "        return X, encoded_dict_list, nan_dict\n",
    "\n",
    "    def fit_resample(self, X, y):\n",
    "\n",
    "        X_cat_encoded, encoded_dict_list, nan_dict = self.cat_corr_pandas(X.iloc[:,np.asarray(self.categorical_features)],  y, target_column='fake_cat1', target_value=1)\n",
    "\n",
    "        X_cat_encoded = np.array(X_cat_encoded)\n",
    "        y = np.ravel(y)\n",
    "        X = np.array(X)\n",
    "\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        target_stats = dict(zip(unique, counts))\n",
    "        n_sample_majority = max(target_stats.values())\n",
    "        class_majority = max(target_stats, key=target_stats.get)\n",
    "        sampling_strategy = {key: n_sample_majority - value for (key, value) in target_stats.items() if key != class_majority}\n",
    "\n",
    "        n_features_ = X.shape[1]\n",
    "        categorical_features = np.asarray(self.categorical_features)\n",
    "        if categorical_features.dtype.name == 'bool':\n",
    "            categorical_features_ = np.flatnonzero(categorical_features)\n",
    "        else:\n",
    "            if any([cat not in np.arange(n_features_) for cat in categorical_features]):\n",
    "                raise ValueError('Some of the categorical indices are out of range. Indices'\n",
    "                            ' should be between 0 and {}'.format(n_features_))\n",
    "            categorical_features_ = categorical_features\n",
    "\n",
    "        continuous_features_ = np.setdiff1d(np.arange(n_features_),categorical_features_)\n",
    "\n",
    "        target_stats = Counter(y)\n",
    "        class_minority = min(target_stats, key=target_stats.get)\n",
    "\n",
    "        X_continuous = X[:, continuous_features_]\n",
    "        X_continuous = check_array(X_continuous, accept_sparse=['csr', 'csc'])\n",
    "        X_minority = _safe_indexing(X_continuous, np.flatnonzero(y == class_minority))\n",
    "\n",
    "        if sparse.issparse(X):\n",
    "            if X.format == 'csr':\n",
    "                _, var = sparsefuncs_fast.csr_mean_variance_axis0(X_minority)\n",
    "            else:\n",
    "                _, var = sparsefuncs_fast.csc_mean_variance_axis0(X_minority)\n",
    "        else:\n",
    "            var = X_minority.var(axis=0)\n",
    "        median_std_ = np.median(np.sqrt(var))\n",
    "\n",
    "        X_categorical = X[:, categorical_features_]\n",
    "        X_copy = np.hstack((X_continuous, X_categorical))\n",
    "        \n",
    "        # X_cat_encoded = X_cat_encoded * median_std_\n",
    "        X_cat_encoded = X_cat_encoded * 0\n",
    "        X_encoded = np.hstack((X_continuous, X_cat_encoded))\n",
    "        X_resampled = X_encoded.copy()\n",
    "        y_resampled = y.copy()\n",
    "\n",
    "\n",
    "        for class_sample, n_samples in sampling_strategy.items():\n",
    "            if n_samples == 0:\n",
    "                continue\n",
    "            target_class_indices = np.flatnonzero(y == class_sample)\n",
    "            X_class = _safe_indexing(X_encoded, target_class_indices)\n",
    "            nn_k_ = self.chk_neighbors(5, 1)\n",
    "            nn_k_.fit(X_class)\n",
    "            nns = nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
    "\n",
    "            X_new, y_new = self.make_samples(X_class, y.dtype, class_sample, X_class, nns, n_samples, continuous_features_, 0)\n",
    "            if sparse.issparse(X_new):\n",
    "                X_resampled = sparse.vstack([X_resampled, X_new])\n",
    "                sparse_func = 'tocsc' if X.format == 'csc' else 'tocsr'\n",
    "                X_resampled = getattr(X_resampled, sparse_func)()\n",
    "            else:\n",
    "                X_resampled = np.vstack((X_resampled, X_new))\n",
    "            y_resampled = np.hstack((y_resampled, y_new))\n",
    "\n",
    "        X_resampled_copy = X_resampled.copy()\n",
    "        i = 0\n",
    "        for col in range(continuous_features_.size, X.shape[1]):\n",
    "            encoded_dict = encoded_dict_list[i]\n",
    "            i = i + 1\n",
    "            for key, value in encoded_dict.items():\n",
    "                X_resampled_copy[:, col] = np.where(np.round(X_resampled_copy[:, col], 4) == np.round(value * median_std_, 4), key, X_resampled_copy[:, col])\n",
    "\n",
    "        for key, value in nan_dict.items():\n",
    "            for item in value:\n",
    "                X_resampled_copy[item, continuous_features_.size + key] = X_copy[item, continuous_features_.size + key]\n",
    "\n",
    "               \n",
    "        X_resampled = X_resampled_copy   \n",
    "        indices_reordered = np.argsort(np.hstack((continuous_features_, categorical_features_)))\n",
    "        if sparse.issparse(X_resampled):\n",
    "            col_indices = X_resampled.indices.copy()\n",
    "            for idx, col_idx in enumerate(indices_reordered):\n",
    "                mask = X_resampled.indices == col_idx\n",
    "                col_indices[mask] = idx\n",
    "            X_resampled.indices = col_indices\n",
    "        else:\n",
    "            X_resampled = X_resampled[:, indices_reordered]\n",
    "        return X_resampled, y_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b90ee31-9838-40db-bf44-c17aaca57465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from SMOTE_ENC import mSMOTENC\n",
    "def new_method(dataset):\n",
    "    mcc = []\n",
    "    f1 = []\n",
    "    auc_a = []\n",
    "    gmean = []\n",
    "    times = []\n",
    "    y_preds = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    traFiles = sorted(glob.glob(base_path+dataset+'/*tra.xlsx'))\n",
    "    tstFiles = sorted(glob.glob(base_path+dataset+'/*tst.xlsx'))\n",
    "    for traPath, tstPath in zip(traFiles, tstFiles):\n",
    "        print(traPath)\n",
    "\n",
    "        df_train = pd.read_excel(traPath)\n",
    "        df_test = pd.read_excel(tstPath)\n",
    "\n",
    "        x_train= df_train.iloc[:, 1:-1]\n",
    "        y_train = df_train.iloc[:, -1]\n",
    "        x_test= df_test.iloc[:, 1:-1]\n",
    "        y_test = df_test.iloc[:, -1]\n",
    "\n",
    "        #####\n",
    "        # for some dataset get error Unknown label type: 'unknown'\n",
    "        y_train = y_train.astype('int')\n",
    "        y_test = y_test.astype('int')\n",
    "\n",
    "        print(x_train.shape[1])\n",
    "        x_train['fake_cat1'] = 0    \n",
    "        st = time.time()\n",
    "\n",
    "        msmotenc = mSMOTENC(categorical_features=[x_train.shape[1]-1])\n",
    "        X_resampled, y_resampled = msmotenc.fit_resample(x_train, y_train)\n",
    "        print(X_resampled.shape, np.unique(X_resampled[:,x_train.shape[1]-1]))    \n",
    "        print(X_resampled[:,:x_train.shape[1]-1].shape)\n",
    "        x_train = X_resampled[:,:x_train.shape[1]-1]\n",
    "        # x_train = X_resampled\n",
    "        y_train = y_resampled\n",
    "\n",
    "        clf = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "\n",
    "        clf.fit(x_train, y_train)\n",
    "\n",
    "        y_pred = clf.predict(x_test) \n",
    "        et = time.time()\n",
    "        y_preds.append(y_pred)\n",
    "        # compute error\n",
    "        mcc.append(matthews_corrcoef(y_test, y_pred))\n",
    "        #--------------------------------\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "        auc_a.append(auc(fpr, tpr))\n",
    "        #--------------------------------\n",
    "        f1.append(f1_score(y_test, y_pred))\n",
    "        #--------------------------------\n",
    "        gmean.append(geometric_mean_score(y_test, y_pred, labels=[1, -1]))\n",
    "\n",
    "        #time of train and test\n",
    "        times.append(et - st)\n",
    "\n",
    "        precisions.append(precision_score(y_test, y_pred))\n",
    "        recalls.append(precision_score(y_test, y_pred))\n",
    "\n",
    "        return {\"precision\": precisions, \"recall\": recalls, \"mcc\": mcc, \"auc\": auc_a, \"f1\": f1, \"gmean\": gmean, \"exe_time\": times, \"y_pred\": y_pred}\n",
    "\n",
    "# new_method('ecoli-0-2-6-7_vs_3-5-5-fold')\n",
    "# new_method(datasets[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac24869e-1075-41d7-aad3-2f787d9ecfd2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "657a62a1-8a16-4a41-b39c-317a7cc85828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd \n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# base_path = 'Datasets/'\n",
    "# # need_to_convert = ['wisconsin-5-fold','new-thyroid2-5-fold','new-thyroid1-5-fold']\n",
    "# # need_to_convert = ['yeast-1_vs_7-5-fold', 'led7digit-0-2-4-5-6-7-8-9_vs_1-5-fold', 'ecoli-0-3-4-6_vs_5-5-fold', 'abalone19-5-', 'abalone19-5-fold']\n",
    "# # need_to_convert = ['ecoli-0-1_vs_5-5-fold','ecoli-0-1-4-7_vs_5-6-5-fold','ecoli-0-3-4-7_vs_5-6-5-fold','glass-0-1-4-6_vs_2-5-fold', 'ecoli-0-4-6_vs_5-5-fold',\n",
    "# #                   'yeast-0-3-5-9_vs_7-8-5-fold','yeast-1-2-8-9_vs_7-5-fold']\n",
    "# need_to_convert = ['yeast-1-2-8-9_vs_7-5-fold']\n",
    "# for needed in need_to_convert:\n",
    "#     for datFile in glob.glob(base_path+needed+'/*.dat'):\n",
    "#         print(datFile)\n",
    "#         df = read_dot_dat_file(datFile)\n",
    "#         df.to_excel(base_path+needed+'/'+datFile.split('/')[-1].split('.')[0]+'.xlsx')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
