{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30c6ba7c-abf0-4879-8daf-64cf03f708e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda install -c conda-forge imbalanced-learn -y\n",
    "# ! pip install nose\n",
    "# ! pip install imbalanced-ensemble           \n",
    "# ! pip install threadpoolctl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "48f205d1-e543-4532-a098-2be27173f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Mimimkh/SMOTE-ENC-code/\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.utils import check_array, _safe_indexing, sparsefuncs_fast, check_X_y, check_random_state\n",
    "from numbers import Integral\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "\n",
    "class mSMOTENC():\n",
    "    \n",
    "    def __init__(self, categorical_features):\n",
    "        self.categorical_features = categorical_features\n",
    "        \n",
    "    def chk_neighbors(self, nn_object, additional_neighbor):\n",
    "        if isinstance(nn_object, Integral):\n",
    "            return NearestNeighbors(n_neighbors=nn_object + additional_neighbor)\n",
    "        elif isinstance(nn_object, KNeighborsMixin):\n",
    "            return clone(nn_object)\n",
    "        else:\n",
    "            raise_isinstance_error(nn_name, [int, KNeighborsMixin], nn_object)     \n",
    "    \n",
    "    def generate_samples(self, X, nn_data, nn_num, rows, cols, steps, continuous_features_,):\n",
    "        rng = check_random_state(42)\n",
    "\n",
    "        diffs = nn_data[nn_num[rows, cols]] - X[rows]\n",
    "\n",
    "        if sparse.issparse(X):\n",
    "            sparse_func = type(X).__name__\n",
    "            steps = getattr(sparse, sparse_func)(steps)\n",
    "            X_new = X[rows] + steps.multiply(diffs)\n",
    "        else:\n",
    "            X_new = X[rows] + steps * diffs \n",
    "\n",
    "        X_new = (X_new.tolil() if sparse.issparse(X_new) else X_new)\n",
    "        # convert to dense array since scipy.sparse doesn't handle 3D\n",
    "        nn_data = (nn_data.toarray() if sparse.issparse(nn_data) else nn_data)\n",
    "        all_neighbors = nn_data[nn_num[rows]]\n",
    "\n",
    "        for idx in range(continuous_features_.size, X.shape[1]):\n",
    "            mode = stats.mode(all_neighbors[:, :, idx], axis = 1)[0]\n",
    "            X_new[:, idx] = np.ravel(mode)\n",
    "\n",
    "        return X_new\n",
    "    \n",
    "    def make_samples(self, X, y_dtype, y_type, nn_data, nn_num, n_samples, continuous_features_, step_size=1.0):\n",
    "        random_state = check_random_state(42)\n",
    "        samples_indices = random_state.randint(low=0, high=len(nn_num.flatten()), size=n_samples)    \n",
    "        steps = step_size * random_state.uniform(size=n_samples)[:, np.newaxis]\n",
    "        rows = np.floor_divide(samples_indices, nn_num.shape[1])\n",
    "        cols = np.mod(samples_indices, nn_num.shape[1])\n",
    "\n",
    "        X_new = self.generate_samples(X, nn_data, nn_num, rows, cols, steps, continuous_features_)\n",
    "        y_new = np.full(n_samples, fill_value=y_type, dtype=y_dtype)\n",
    "        \n",
    "        return X_new, y_new\n",
    "    \n",
    "    def cat_corr_pandas(self, X, target_df, target_column, target_value):\n",
    "    # X has categorical columns\n",
    "        categorical_columns = list(X.columns)\n",
    "        X = pd.concat([X, target_df], axis=1)\n",
    "\n",
    "        # filter X for target value\n",
    "        is_target = X.loc[:, target_column] == target_value\n",
    "        X_filtered = X.loc[is_target, :]\n",
    "\n",
    "        X_filtered.drop(target_column, axis=1, inplace=True)\n",
    "\n",
    "        # get columns in X\n",
    "        nrows = len(X)\n",
    "        encoded_dict_list = []\n",
    "        nan_dict = dict({})\n",
    "        c = 0\n",
    "        imb_ratio = len(X_filtered)/len(X)\n",
    "        OE_dict = {}\n",
    "        \n",
    "        for column in categorical_columns:\n",
    "            for level in list(X.loc[:, column].unique()):\n",
    "                \n",
    "                # filter rows where level is present\n",
    "                row_level_filter = X.loc[:, column] == level\n",
    "                rows_in_level = len(X.loc[row_level_filter, :])\n",
    "                \n",
    "                # number of rows in level where target is 1\n",
    "                O = len(X.loc[is_target & row_level_filter, :])\n",
    "                E = rows_in_level * imb_ratio\n",
    "                # Encoded value = chi, i.e. (observed - expected)/expected\n",
    "                ENC = (O - E+0.000000000000000000000000000000000000000000000000000000001) / (E+0.000000000000000000000000000000000000000000000000000000001)\n",
    "                # ENC = (O - E+0.0000000000001) / (E+0.0000000000001)\n",
    "                OE_dict[level] = ENC\n",
    "                \n",
    "            encoded_dict_list.append(OE_dict)\n",
    "\n",
    "            X.loc[:, column] = X[column].map(OE_dict)\n",
    "            # print(f'X.loc[:, {column}]', X.loc[:, column])\n",
    "            # nan_idx_array = np.ravel(np.argwhere(np.isnan(X.loc[:, column])))\n",
    "            nan_idx_array = np.array([0])\n",
    "            if len(nan_idx_array) > 0 :\n",
    "                nan_dict[c] = nan_idx_array\n",
    "            c = c + 1\n",
    "            X.loc[:, column].fillna(-1, inplace = True)\n",
    "                \n",
    "        X.drop(target_column, axis=1, inplace=True)\n",
    "        return X, encoded_dict_list, nan_dict\n",
    "\n",
    "    def fit_resample(self, X, y):\n",
    "\n",
    "        X_cat_encoded, encoded_dict_list, nan_dict = self.cat_corr_pandas(X.iloc[:,np.asarray(self.categorical_features)],  y, target_column='fake_cat1', target_value=1)\n",
    "\n",
    "        X_cat_encoded = np.array(X_cat_encoded)\n",
    "        y = np.ravel(y)\n",
    "        X = np.array(X)\n",
    "\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        target_stats = dict(zip(unique, counts))\n",
    "        n_sample_majority = max(target_stats.values())\n",
    "        class_majority = max(target_stats, key=target_stats.get)\n",
    "        sampling_strategy = {key: n_sample_majority - value for (key, value) in target_stats.items() if key != class_majority}\n",
    "\n",
    "        n_features_ = X.shape[1]\n",
    "        categorical_features = np.asarray(self.categorical_features)\n",
    "        if categorical_features.dtype.name == 'bool':\n",
    "            categorical_features_ = np.flatnonzero(categorical_features)\n",
    "        else:\n",
    "            if any([cat not in np.arange(n_features_) for cat in categorical_features]):\n",
    "                raise ValueError('Some of the categorical indices are out of range. Indices'\n",
    "                            ' should be between 0 and {}'.format(n_features_))\n",
    "            categorical_features_ = categorical_features\n",
    "\n",
    "        continuous_features_ = np.setdiff1d(np.arange(n_features_),categorical_features_)\n",
    "\n",
    "        target_stats = Counter(y)\n",
    "        class_minority = min(target_stats, key=target_stats.get)\n",
    "\n",
    "        X_continuous = X[:, continuous_features_]\n",
    "        X_continuous = check_array(X_continuous, accept_sparse=['csr', 'csc'])\n",
    "        X_minority = _safe_indexing(X_continuous, np.flatnonzero(y == class_minority))\n",
    "\n",
    "        if sparse.issparse(X):\n",
    "            if X.format == 'csr':\n",
    "                _, var = sparsefuncs_fast.csr_mean_variance_axis0(X_minority)\n",
    "            else:\n",
    "                _, var = sparsefuncs_fast.csc_mean_variance_axis0(X_minority)\n",
    "        else:\n",
    "            var = X_minority.var(axis=0)\n",
    "        median_std_ = np.median(np.sqrt(var))\n",
    "\n",
    "        X_categorical = X[:, categorical_features_]\n",
    "        X_copy = np.hstack((X_continuous, X_categorical))\n",
    "        \n",
    "        # X_cat_encoded = X_cat_encoded * median_std_\n",
    "        X_cat_encoded = X_cat_encoded * 0\n",
    "        X_encoded = np.hstack((X_continuous, X_cat_encoded))\n",
    "        X_resampled = X_encoded.copy()\n",
    "        y_resampled = y.copy()\n",
    "\n",
    "\n",
    "        for class_sample, n_samples in sampling_strategy.items():\n",
    "            if n_samples == 0:\n",
    "                continue\n",
    "            target_class_indices = np.flatnonzero(y == class_sample)\n",
    "            X_class = _safe_indexing(X_encoded, target_class_indices)\n",
    "            nn_k_ = self.chk_neighbors(5, 1)\n",
    "            nn_k_.fit(X_class)\n",
    "            nns = nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
    "\n",
    "            X_new, y_new = self.make_samples(X_class, y.dtype, class_sample, X_class, nns, n_samples, continuous_features_, 0)\n",
    "            if sparse.issparse(X_new):\n",
    "                X_resampled = sparse.vstack([X_resampled, X_new])\n",
    "                sparse_func = 'tocsc' if X.format == 'csc' else 'tocsr'\n",
    "                X_resampled = getattr(X_resampled, sparse_func)()\n",
    "            else:\n",
    "                X_resampled = np.vstack((X_resampled, X_new))\n",
    "            y_resampled = np.hstack((y_resampled, y_new))\n",
    "\n",
    "        X_resampled_copy = X_resampled.copy()\n",
    "        i = 0\n",
    "        for col in range(continuous_features_.size, X.shape[1]):\n",
    "            encoded_dict = encoded_dict_list[i]\n",
    "            i = i + 1\n",
    "            for key, value in encoded_dict.items():\n",
    "                X_resampled_copy[:, col] = np.where(np.round(X_resampled_copy[:, col], 4) == np.round(value * median_std_, 4), key, X_resampled_copy[:, col])\n",
    "\n",
    "        for key, value in nan_dict.items():\n",
    "            for item in value:\n",
    "                X_resampled_copy[item, continuous_features_.size + key] = X_copy[item, continuous_features_.size + key]\n",
    "\n",
    "               \n",
    "        X_resampled = X_resampled_copy   \n",
    "        indices_reordered = np.argsort(np.hstack((continuous_features_, categorical_features_)))\n",
    "        if sparse.issparse(X_resampled):\n",
    "            col_indices = X_resampled.indices.copy()\n",
    "            for idx, col_idx in enumerate(indices_reordered):\n",
    "                mask = X_resampled.indices == col_idx\n",
    "                col_indices[mask] = idx\n",
    "            X_resampled.indices = col_indices\n",
    "        else:\n",
    "            X_resampled = X_resampled[:, indices_reordered]\n",
    "        return X_resampled, y_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "384b570b-8838-4324-9b8f-2a8b9774b119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets/led7digit-0-2-4-5-6-7-8-9_vs_1-5-fold/led7digit-0-2-4-5-6-7-8-9_vs_1-5-1tra.xlsx\n",
      "7\n",
      "(650, 8) [0.]\n",
      "(650, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': [0.7272727272727273],\n",
       " 'recall': [0.7272727272727273],\n",
       " 'mcc': [0.8368612192811304],\n",
       " 'auc': [0.9814814814814815],\n",
       " 'f1': [0.8421052631578948],\n",
       " 'gmean': [0.9813067629253163],\n",
       " 'exe_time': [0.10255599021911621],\n",
       " 'y_pred': array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1])}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from SMOTE_ENC import mSMOTENC\n",
    "\n",
    "\n",
    "def new_method(dataset):\n",
    "    mcc = []\n",
    "    f1 = []\n",
    "    auc_a = []\n",
    "    gmean = []\n",
    "    times = []\n",
    "    y_preds = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    traFiles = sorted(glob.glob(base_path+dataset+'/*tra.xlsx'))\n",
    "    tstFiles = sorted(glob.glob(base_path+dataset+'/*tst.xlsx'))\n",
    "    for traPath, tstPath in zip(traFiles, tstFiles):\n",
    "        print(traPath)\n",
    "\n",
    "        df_train = pd.read_excel(traPath)\n",
    "        df_test = pd.read_excel(tstPath)\n",
    "\n",
    "        x_train= df_train.iloc[:, 1:-1]\n",
    "        y_train = df_train.iloc[:, -1]\n",
    "        x_test= df_test.iloc[:, 1:-1]\n",
    "        y_test = df_test.iloc[:, -1]\n",
    "\n",
    "        #####\n",
    "        # for some dataset get error Unknown label type: 'unknown'\n",
    "        y_train = y_train.astype('int')\n",
    "        y_test = y_test.astype('int')\n",
    "\n",
    "        print(x_train.shape[1])\n",
    "        x_train['fake_cat1'] = 0    \n",
    "        st = time.time()\n",
    "\n",
    "        msmotenc = mSMOTENC(categorical_features=[x_train.shape[1]-1])\n",
    "        X_resampled, y_resampled = msmotenc.fit_resample(x_train, y_train)\n",
    "        print(X_resampled.shape, np.unique(X_resampled[:,x_train.shape[1]-1]))    \n",
    "        print(X_resampled[:,:x_train.shape[1]-1].shape)\n",
    "        x_train = X_resampled[:,:x_train.shape[1]-1]\n",
    "        # x_train = X_resampled\n",
    "        y_train = y_resampled\n",
    "\n",
    "        clf = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "\n",
    "        clf.fit(x_train, y_train)\n",
    "\n",
    "        y_pred = clf.predict(x_test) \n",
    "        et = time.time()\n",
    "        y_preds.append(y_pred)\n",
    "        # compute error\n",
    "        mcc.append(matthews_corrcoef(y_test, y_pred))\n",
    "        #--------------------------------\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "        auc_a.append(auc(fpr, tpr))\n",
    "        #--------------------------------\n",
    "        f1.append(f1_score(y_test, y_pred))\n",
    "        #--------------------------------\n",
    "        gmean.append(geometric_mean_score(y_test, y_pred, labels=[1, -1]))\n",
    "\n",
    "        #time of train and test\n",
    "        times.append(et - st)\n",
    "\n",
    "        precisions.append(precision_score(y_test, y_pred))\n",
    "        recalls.append(precision_score(y_test, y_pred))\n",
    "\n",
    "        return {\"precision\": precisions, \"recall\": recalls, \"mcc\": mcc, \"auc\": auc_a, \"f1\": f1, \"gmean\": gmean, \"exe_time\": times, \"y_pred\": y_pred}\n",
    "\n",
    "# new_method('ecoli-0-2-6-7_vs_3-5-5-fold')\n",
    "new_method(datasets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9a4737c5-11b3-4729-95c5-fd7458fe1d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yeast-1_vs_7-5-fold',\n",
       " 'led7digit-0-2-4-5-6-7-8-9_vs_1-5-fold',\n",
       " 'ecoli-0-6-7_vs_3-5-5-fold',\n",
       " 'glass-0-4_vs_5-5-fold',\n",
       " 'ecoli-0-2-6-7_vs_3-5-5-fold',\n",
       " 'yeast-0-2-5-6_vs_3-7-8-9-5-fold',\n",
       " 'yeast-0-5-6-7-9_vs_4-5-fold',\n",
       " 'ecoli-0-3-4-6_vs_5-5-fold',\n",
       " 'yeast5-5-fold',\n",
       " 'yeast-2_vs_4-5-fold',\n",
       " 'ecoli-0-1-4-6_vs_5-5-fold',\n",
       " 'shuttle-c2-vs-c4-5-fold',\n",
       " 'ecoli-0-1_vs_5-5-fold',\n",
       " 'yeast4-5-fold',\n",
       " 'abalone19-5-',\n",
       " 'glass-0-1-6_vs_5-5-fold',\n",
       " 'ecoli-0-1-4-7_vs_2-3-5-6-5-fold',\n",
       " 'ecoli-0-1-4-7_vs_5-6-5-fold',\n",
       " 'ecoli-0-3-4-7_vs_5-6-5-fold',\n",
       " 'yeast-2_vs_8-5-fold',\n",
       " 'ecoli-0-1-3-7_vs_2-6-5-fold',\n",
       " 'yeast6-5-fold',\n",
       " 'pima-5-fold',\n",
       " 'glass-0-1-4-6_vs_2-5-fold',\n",
       " 'abalone9-18-5-fold',\n",
       " 'abalone19-5-fold',\n",
       " 'glass4-5-fold',\n",
       " 'ecoli-0-2-3-4_vs_5-5-fold',\n",
       " 'vehicle2-5-fold',\n",
       " 'glass5-5-fold',\n",
       " 'ecoli-0_vs_1-5-fold',\n",
       " 'ecoli-0-6-7_vs_5-5-fold',\n",
       " 'glass-0-1-5_vs_2-5-fold',\n",
       " 'shuttle-c0-vs-c4-5-fold',\n",
       " 'ecoli-0-4-6_vs_5-5-fold',\n",
       " 'new-thyroid1-5-fold',\n",
       " 'ecoli4-5-fold',\n",
       " 'glass-0-6_vs_5-5-fold',\n",
       " 'segment0-5-fold',\n",
       " 'glass2-5-fold',\n",
       " 'yeast-0-3-5-9_vs_7-8-5-fold',\n",
       " 'yeast-1-4-5-8_vs_7-5-fold',\n",
       " 'wisconsin-5-fold',\n",
       " 'page-blocks0-5-fold',\n",
       " 'haberman-5-fold',\n",
       " 'page-blocks-1-3_vs_4-5-fold',\n",
       " 'yeast-0-2-5-7-9_vs_3-6-8-5-fold',\n",
       " 'vowel0-5-fold',\n",
       " 'led7digit-0-2-4-5-6-7-8-9_vs_1-5-',\n",
       " 'ecoli-0-1_vs_2-3-5-5-fold',\n",
       " 'glass-0-1-6_vs_2-5-fold',\n",
       " 'yeast-1-2-8-9_vs_7-5-fold',\n",
       " 'ecoli-0-3-4_vs_5-5-fold',\n",
       " 'new-thyroid2-5-fold']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a97ceea7-d25a-4b9f-9da9-66198967610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def read_dot_dat_file(path):\n",
    "    datContent = [i.strip().split() for i in open(path).readlines()]\n",
    "    r = re.compile(\"@inputs.*\")\n",
    "    _at_data = datContent.index(['@data'])\n",
    "    assert datContent[0][0] == '@relation'\n",
    "    assert datContent[_at_data-1][0] == '@outputs'\n",
    "    assert datContent[_at_data-2][0] == '@inputs'\n",
    "    print(datContent[_at_data-3][2:])\n",
    "    assert len(datContent[_at_data-3][2:]) == 2   # Two Class\n",
    "\n",
    "    col_names = datContent[_at_data-2][1:]\n",
    "    col_names.append(datContent[_at_data-1][1])\n",
    "    \n",
    "    df = pd.read_csv(path, skiprows=_at_data+1, names=col_names, sep=r', ', engine='python')\n",
    "    # df = pd.read_csv(path, skiprows=_at_data+1, names=col_names, sep=\", \", engine='python')\n",
    "\n",
    "    class1 = datContent[_at_data-3][2:][0].replace(\"{\",\"\").replace(\",\",\"\")\n",
    "    class2 = datContent[_at_data-3][2:][1].replace(\"}\",\"\").replace(\",\",\"\")\n",
    "\n",
    "    df['Class'] = df['Class'].replace({class1: 1, class2: -1})\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5780530b-2810-424b-8feb-0a847faf8ca1",
   "metadata": {},
   "source": [
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af89ab8a-ec78-4dbd-a01e-3d02ce85b6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd \n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# base_path = 'Datasets/'\n",
    "# # need_to_convert = ['wisconsin-5-fold','new-thyroid2-5-fold','new-thyroid1-5-fold']\n",
    "# # need_to_convert = ['yeast-1_vs_7-5-fold', 'led7digit-0-2-4-5-6-7-8-9_vs_1-5-fold', 'ecoli-0-3-4-6_vs_5-5-fold', 'abalone19-5-', 'abalone19-5-fold']\n",
    "# # need_to_convert = ['ecoli-0-1_vs_5-5-fold','ecoli-0-1-4-7_vs_5-6-5-fold','ecoli-0-3-4-7_vs_5-6-5-fold','glass-0-1-4-6_vs_2-5-fold', 'ecoli-0-4-6_vs_5-5-fold',\n",
    "# #                   'yeast-0-3-5-9_vs_7-8-5-fold','yeast-1-2-8-9_vs_7-5-fold']\n",
    "# need_to_convert = ['yeast-1-2-8-9_vs_7-5-fold']\n",
    "# for needed in need_to_convert:\n",
    "#     for datFile in glob.glob(base_path+needed+'/*.dat'):\n",
    "#         print(datFile)\n",
    "#         df = read_dot_dat_file(datFile)\n",
    "#         df.to_excel(base_path+needed+'/'+datFile.split('/')[-1].split('.')[0]+'.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b312338-1179-4c80-bf40-7a14b9d12047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import random  \n",
    "import pandas as pd \n",
    "import random\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc,f1_score, matthews_corrcoef, precision_score, recall_score\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "\n",
    "\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from imbalanced_ensemble.ensemble import SMOTEBoostClassifier\n",
    "from maatpy.classifiers import AdaCost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f6a92d6-54cc-46a5-929a-3918bc55dabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_path = 'Datasets/'\n",
    "datasets  = os.listdir(base_path) \n",
    "classifiers = {\"RUS\": RUSBoostClassifier(random_state=0, algorithm='SAMME', base_estimator=DecisionTreeClassifier(max_depth=10)),\n",
    "              \"SMOTE\": SMOTEBoostClassifier(estimator=DecisionTreeClassifier(max_depth=10), n_estimators = 100, algorithm='SAMME', random_state=0),\n",
    "              \"Ada1\": AdaCost(base_estimator=DecisionTreeClassifier(max_depth=10), n_estimators=100, algorithm='adac1',random_state=0),\n",
    "              \"AdaCost\": AdaCost(base_estimator=DecisionTreeClassifier(max_depth=10), n_estimators=100, algorithm='adacost',random_state=0)}\n",
    "            \n",
    "\n",
    "def clasify(dataset, classifier):\n",
    "    mcc = []\n",
    "    f1 = []\n",
    "    auc_a = []\n",
    "    gmean = []\n",
    "    times = []\n",
    "    y_preds = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    traFiles = sorted(glob.glob(base_path+dataset+'/*tra.xlsx'))\n",
    "    tstFiles = sorted(glob.glob(base_path+dataset+'/*tst.xlsx'))\n",
    "    for traPath, tstPath in zip(traFiles, tstFiles):\n",
    "        print(traPath)\n",
    "        \n",
    "        df_train = pd.read_excel(traPath)\n",
    "        df_test = pd.read_excel(tstPath)\n",
    "\n",
    "        x_train= df_train.iloc[:, 1:-1]\n",
    "        y_train = df_train.iloc[:, -1]\n",
    "        x_test= df_test.iloc[:, 1:-1]\n",
    "        y_test = df_test.iloc[:, -1]\n",
    "        \n",
    "        #####\n",
    "        # for some dataset get error Unknown label type: 'unknown'\n",
    "        y_train = y_train.astype('int')\n",
    "        y_test = y_test.astype('int')\n",
    "        \n",
    "        st = time.time()\n",
    "        clf = classifiers[classifier]\n",
    "\n",
    "        clf.fit(x_train, y_train)\n",
    "\n",
    "        y_pred = clf.predict(x_test) \n",
    "        et = time.time()\n",
    "        y_preds.append(y_pred)\n",
    "        # compute error\n",
    "        mcc.append(matthews_corrcoef(y_test, y_pred))\n",
    "        #--------------------------------\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "        auc_a.append(auc(fpr, tpr))\n",
    "        #--------------------------------\n",
    "        f1.append(f1_score(y_test, y_pred))\n",
    "        #--------------------------------\n",
    "        gmean.append(geometric_mean_score(y_test, y_pred, labels=[1, -1]))\n",
    "        \n",
    "        #time of train and test\n",
    "        times.append(et - st)\n",
    "        \n",
    "        precisions.append(precision_score(y_test, y_pred))\n",
    "        recalls.append(precision_score(y_test, y_pred))\n",
    "        \n",
    "    return {\"precision\": precisions, \"recall\": recalls, \"mcc\": mcc, \"auc\": auc_a, \"f1\": f1, \"gmean\": gmean, \"exe_time\": times, \"y_pred\": y_pred}\n",
    "\n",
    "# aaaaaa =  clasify(datasets[0], 'Ada1')\n",
    "# aaaaaa =  clasify('yeast-1-2-8-9_vs_7-5-fold', 'RUS')\n",
    "# aaaaaa\n",
    "# clasify('yeast4-5-fold', 'OUBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9f0a8692-aeb2-4fcd-a2d8-bc54829958fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #temp \n",
    "for i in range(len(datasets)):\n",
    "    if not os.path.isdir('./Results/'+datasets[i]):\n",
    "        os.mkdir('./Results/'+datasets[i])\n",
    "    for classifier in ['STOMEENC']:\n",
    "        if not os.path.isdir('./Results/'+datasets[i]+'/'+classifier):\n",
    "            os.mkdir('./Results/'+datasets[i]+'/'+classifier)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c4e88568-2e7f-473c-b2a4-e2737994cc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_to_files(dataset, classifier, dict_res):\n",
    "    np.save('./Results/'+dataset+'/'+classifier+'/mcc.npy',dict_res['mcc'])\n",
    "    np.save('./Results/'+dataset+'/'+classifier+'/auc.npy',dict_res['auc'])\n",
    "    np.save('./Results/'+dataset+'/'+classifier+'/f1.npy',dict_res['f1'])\n",
    "    np.save('./Results/'+dataset+'/'+classifier+'/gmean.npy',dict_res['gmean'])\n",
    "    np.save('./Results/'+dataset+'/'+classifier+'/exe_time.npy',dict_res['exe_time'])\n",
    "    np.save('./Results/'+dataset+'/'+classifier+'/y_pred.npy',dict_res['y_pred'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8f9ec686-ac23-4236-ba0e-47c56c1cb30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets/yeast-1_vs_7-5-fold/yeast-1_vs_7-5-1tra.xlsx\n",
      "7\n",
      "(686, 8) [0.]\n",
      "(686, 7)\n",
      "Datasets/led7digit-0-2-4-5-6-7-8-9_vs_1-5-fold/led7digit-0-2-4-5-6-7-8-9_vs_1-5-1tra.xlsx\n",
      "7\n",
      "(650, 8) [0.]\n",
      "(650, 7)\n",
      "Datasets/ecoli-0-6-7_vs_3-5-5-fold/ecoli-0-6-7_vs_3-5-5-1tra.xlsx\n",
      "6\n",
      "(320, 7) [0.]\n",
      "(320, 6)\n",
      "Datasets/glass-0-4_vs_5-5-fold/glass-0-4_vs_5-5-1tra.xlsx\n",
      "8\n",
      "(132, 9) [0.]\n",
      "(132, 8)\n",
      "Datasets/ecoli-0-2-6-7_vs_3-5-5-fold/ecoli-0-2-6-7_vs_3-5-5-1tra.xlsx\n",
      "6\n",
      "(324, 7) [0.]\n",
      "(324, 6)\n",
      "Datasets/yeast-0-2-5-6_vs_3-7-8-9-5-fold/yeast-0-2-5-6_vs_3-7-8-9-5-1tra.xlsx\n",
      "7\n",
      "(1448, 8) [0.]\n",
      "(1448, 7)\n",
      "Datasets/yeast-0-5-6-7-9_vs_4-5-fold/yeast-0-5-6-7-9_vs_4-5-1tra.xlsx\n",
      "7\n",
      "(764, 8) [0.]\n",
      "(764, 7)\n",
      "Datasets/ecoli-0-3-4-6_vs_5-5-fold/ecoli-0-3-4-6_vs_5-5-1tra.xlsx\n",
      "7\n",
      "(296, 8) [0.]\n",
      "(296, 7)\n",
      "Datasets/yeast5-5-fold/yeast5-5-1tra.xlsx\n",
      "7\n",
      "(2304, 8) [0.]\n",
      "(2304, 7)\n",
      "Datasets/yeast-2_vs_4-5-fold/yeast-2_vs_4-5-1tra.xlsx\n",
      "7\n",
      "(742, 8) [0.]\n",
      "(742, 7)\n",
      "Datasets/ecoli-0-1-4-6_vs_5-5-fold/ecoli-0-1-4-6_vs_5-5-1tra.xlsx\n",
      "6\n",
      "(416, 7) [0.]\n",
      "(416, 6)\n",
      "Datasets/shuttle-c2-vs-c4-5-fold/shuttle-c2-vs-c4-5-1tra.xlsx\n",
      "8\n",
      "error ===>  shuttle-c2-vs-c4-5-fold\n",
      "Datasets/ecoli-0-1_vs_5-5-fold/ecoli-0-1_vs_5-5-1tra.xlsx\n",
      "6\n",
      "(352, 7) [0.]\n",
      "(352, 6)\n",
      "Datasets/yeast4-5-fold/yeast4-5-1tra.xlsx\n",
      "7\n",
      "(2294, 8) [0.]\n",
      "(2294, 7)\n",
      "Datasets/abalone19-5-/abalone19-5-1tra.xlsx\n",
      "8\n",
      "(6626, 9) [0.]\n",
      "(6626, 8)\n",
      "Datasets/glass-0-1-6_vs_5-5-fold/glass-0-1-6_vs_5-5-1tra.xlsx\n",
      "8\n",
      "(280, 9) [0.]\n",
      "(280, 8)\n",
      "Datasets/ecoli-0-1-4-7_vs_2-3-5-6-5-fold/ecoli-0-1-4-7_vs_2-3-5-6-5-1tra.xlsx\n",
      "6\n",
      "(490, 7) [0.]\n",
      "(490, 6)\n",
      "Datasets/ecoli-0-1-4-7_vs_5-6-5-fold/ecoli-0-1-4-7_vs_5-6-5-1tra.xlsx\n",
      "6\n",
      "(490, 7) [0.]\n",
      "(490, 6)\n",
      "Datasets/ecoli-0-3-4-7_vs_5-6-5-fold/ecoli-0-3-4-7_vs_5-6-5-1tra.xlsx\n",
      "7\n",
      "(370, 8) [0.]\n",
      "(370, 7)\n",
      "Datasets/yeast-2_vs_8-5-fold/yeast-2_vs_8-5-1tra.xlsx\n",
      "7\n",
      "(738, 8) [0.]\n",
      "(738, 7)\n",
      "Datasets/ecoli-0-1-3-7_vs_2-6-5-fold/ecoli-0-1-3-7_vs_2-6-5-1tra.xlsx\n",
      "6\n",
      "error ===>  ecoli-0-1-3-7_vs_2-6-5-fold\n",
      "Datasets/yeast6-5-fold/yeast6-5-1tra.xlsx\n",
      "7\n",
      "(2318, 8) [0.]\n",
      "(2318, 7)\n",
      "Datasets/pima-5-fold/pima-5-1tra.xlsx\n",
      "8\n",
      "(800, 9) [0.]\n",
      "(800, 8)\n",
      "Datasets/glass-0-1-4-6_vs_2-5-fold/glass-0-1-4-6_vs_2-5-1tra.xlsx\n",
      "9\n",
      "(302, 10) [0.]\n",
      "(302, 9)\n",
      "Datasets/abalone9-18-5-fold/abalone9-18-5-1tra.xlsx\n",
      "6\n",
      "(276, 7) [0.]\n",
      "(276, 6)\n",
      "Datasets/abalone19-5-fold/abalone19-5-1tra.xlsx\n",
      "8\n",
      "(6626, 9) [0.]\n",
      "(6626, 8)\n",
      "Datasets/glass4-5-fold/glass4-5-1tra.xlsx\n",
      "8\n",
      "(322, 9) [0.]\n",
      "(322, 8)\n",
      "Datasets/ecoli-0-2-3-4_vs_5-5-fold/ecoli-0-2-3-4_vs_5-5-1tra.xlsx\n",
      "6\n",
      "(290, 7) [0.]\n",
      "(290, 6)\n",
      "Datasets/vehicle2-5-fold/vehicle2-5-1tra.xlsx\n",
      "18\n",
      "(1004, 19) [0.]\n",
      "(1004, 18)\n",
      "Datasets/glass5-5-fold/glass5-5-1tra.xlsx\n",
      "8\n",
      "(328, 9) [0.]\n",
      "(328, 8)\n",
      "Datasets/ecoli-0_vs_1-5-fold/ecoli-0_vs_1-5-1tra.xlsx\n",
      "7\n",
      "(228, 8) [0.]\n",
      "(228, 7)\n",
      "Datasets/ecoli-0-6-7_vs_5-5-fold/ecoli-0-6-7_vs_5-5-1tra.xlsx\n",
      "5\n",
      "(320, 6) [0.]\n",
      "(320, 5)\n",
      "Datasets/glass-0-1-5_vs_2-5-fold/glass-0-1-5_vs_2-5-1tra.xlsx\n",
      "8\n",
      "(248, 9) [0.]\n",
      "(248, 8)\n",
      "Datasets/shuttle-c0-vs-c4-5-fold/shuttle-c0-vs-c4-5-1tra.xlsx\n",
      "8\n",
      "(2728, 9) [0.]\n",
      "(2728, 8)\n",
      "Datasets/ecoli-0-4-6_vs_5-5-fold/ecoli-0-4-6_vs_5-5-1tra.xlsx\n",
      "6\n",
      "(292, 7) [0.]\n",
      "(292, 6)\n",
      "Datasets/new-thyroid1-5-fold/new-thyroid1-5-1tra.xlsx\n",
      "5\n",
      "(288, 6) [0.]\n",
      "(288, 5)\n",
      "Datasets/ecoli4-5-fold/ecoli4-5-1tra.xlsx\n",
      "6\n",
      "(504, 7) [0.]\n",
      "(504, 6)\n",
      "Datasets/glass-0-6_vs_5-5-fold/glass-0-6_vs_5-5-1tra.xlsx\n",
      "8\n",
      "(158, 9) [0.]\n",
      "(158, 8)\n",
      "Datasets/segment0-5-fold/segment0-5-1tra.xlsx\n",
      "18\n",
      "(3166, 19) [0.]\n",
      "(3166, 18)\n",
      "Datasets/glass2-5-fold/glass2-5-1tra.xlsx\n",
      "8\n",
      "(316, 9) [0.]\n",
      "(316, 8)\n",
      "Datasets/yeast-0-3-5-9_vs_7-8-5-fold/yeast-0-3-5-9_vs_7-8-5-1tra.xlsx\n",
      "8\n",
      "(728, 9) [0.]\n",
      "(728, 8)\n",
      "Datasets/yeast-1-4-5-8_vs_7-5-fold/yeast-1-4-5-8_vs_7-5-1tra.xlsx\n",
      "7\n",
      "(1060, 8) [0.]\n",
      "(1060, 7)\n",
      "Datasets/wisconsin-5-fold/wisconsin-5-1tra.xlsx\n",
      "9\n",
      "(710, 10) [0.]\n",
      "(710, 9)\n",
      "Datasets/page-blocks0-5-fold/page-blocks0-5-1tra.xlsx\n",
      "10\n",
      "(7860, 11) [0.]\n",
      "(7860, 10)\n",
      "Datasets/haberman-5-fold/haberman-5-1tra.xlsx\n",
      "3\n",
      "(360, 4) [0.]\n",
      "(360, 3)\n",
      "Datasets/page-blocks-1-3_vs_4-5-fold/page-blocks-1-3_vs_4-5-1tra.xlsx\n",
      "9\n",
      "(710, 10) [0.]\n",
      "(710, 9)\n",
      "Datasets/yeast-0-2-5-7-9_vs_3-6-8-5-fold/yeast-0-2-5-7-9_vs_3-6-8-5-1tra.xlsx\n",
      "7\n",
      "(1448, 8) [0.]\n",
      "(1448, 7)\n",
      "Datasets/vowel0-5-fold/vowel0-5-1tra.xlsx\n",
      "12\n",
      "(1436, 13) [0.]\n",
      "(1436, 12)\n",
      "Datasets/led7digit-0-2-4-5-6-7-8-9_vs_1-5-/led7digit-0-2-4-5-6-7-8-9_vs_1-5-1tra.xlsx\n",
      "6\n",
      "(650, 7) [0.]\n",
      "(650, 6)\n",
      "Datasets/ecoli-0-1_vs_2-3-5-5-fold/ecoli-0-1_vs_2-3-5-5-1tra.xlsx\n",
      "6\n",
      "(352, 7) [0.]\n",
      "(352, 6)\n",
      "Datasets/glass-0-1-6_vs_2-5-fold/glass-0-1-6_vs_2-5-1tra.xlsx\n",
      "8\n",
      "(280, 9) [0.]\n",
      "(280, 8)\n",
      "Datasets/yeast-1-2-8-9_vs_7-5-fold/yeast-1-2-8-9_vs_7-5-1tra.xlsx\n",
      "7\n",
      "(1466, 8) [0.]\n",
      "(1466, 7)\n",
      "Datasets/ecoli-0-3-4_vs_5-5-fold/ecoli-0-3-4_vs_5-5-1tra.xlsx\n",
      "6\n",
      "(288, 7) [0.]\n",
      "(288, 6)\n",
      "Datasets/new-thyroid2-5-fold/newthyroid2-5-1tra.xlsx\n",
      "5\n",
      "(288, 6) [0.]\n",
      "(288, 5)\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    try:\n",
    "        res_to_files(dataset, 'STOMEENC', new_method(dataset))\n",
    "    except:\n",
    "        print(\"error ===> \", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "be00f465-4b1a-42e6-a951-9c7dc485764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for classifier in classifiers:\n",
    "# #     for dataset in datasets:\n",
    "# #         if not os.path.exists('./Results/'+dataset+'/'+classifier+'/y_pred.npy'):\n",
    "# #             print(classifier,\" => \", dataset, \"=>\", end=' ')\n",
    "# #             # tmp_res = clasify(dataset, classifier)\n",
    "# #             # print(tmp_res['auc'])\n",
    "# #             try:\n",
    "# #                 tmp_res = clasify(dataset, classifier)\n",
    "# #                 res_to_files(dataset, classifier, tmp_res)\n",
    "# #             except:\n",
    "# #                 print(dataset, classifier)\n",
    "\n",
    "# classifier = 'Ada1'\n",
    "# for dataset in datasets:\n",
    "#     if not os.path.exists('./Results/'+dataset+'/'+classifier+'/y_pred.npy'):\n",
    "#         print(classifier,\" => \", dataset, \"=>\", end=' ')\n",
    "#         # try:\n",
    "#         tmp_res = clasify(dataset, classifier)\n",
    "#         res_to_files(dataset, classifier, tmp_res)\n",
    "#         # except:\n",
    "#             # print(dataset, classifier)\n",
    "\n",
    "# # classifier = 'AdaCost'\n",
    "# # dataset = 'abalone19-5-'\n",
    "# # if not os.path.exists('./Results/'+dataset+'/'+classifier+'/y_pred.npy'):\n",
    "# #     print(classifier,\" => \", dataset, \"=>\", end=' ')\n",
    "# #     tmp_res = clasify(dataset, classifier)\n",
    "# #     res_to_files(dataset, classifier, tmp_res)\n",
    "# #     print(tmp_res['auc'])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4dd3c22d-8e51-4549-b055-0166c8ec909c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yeast-1-2-8-9_vs_7-5-fold RUS\n",
      "shuttle-c2-vs-c4-5-fold SMOTE\n",
      "ecoli-0-1-3-7_vs_2-6-5-fold SMOTE\n",
      "yeast4-5-fold Ada1\n",
      "abalone19-5- Ada1\n",
      "abalone19-5-fold Ada1\n",
      "abalone19-5- AdaCost\n",
      "abalone19-5-fold AdaCost\n",
      "shuttle-c2-vs-c4-5-fold STOMEENC\n",
      "ecoli-0-1-3-7_vs_2-6-5-fold STOMEENC\n"
     ]
    }
   ],
   "source": [
    "troubs = {}\n",
    "clssssss = list(classifiers.keys())\n",
    "clssssss.append('STOMEENC')\n",
    "for classifier in clssssss:\n",
    "    for dataset in datasets:\n",
    "        files = glob.glob('./Results/'+dataset+'/'+classifier+'/*.npy')\n",
    "        if len(files) != 6:\n",
    "            if dataset in troubs:\n",
    "                troubs[dataset].append(classifier)\n",
    "            else:\n",
    "                troubs[dataset] = [classifier]\n",
    "            print(dataset, classifier)\n",
    "        \n",
    "# print([dataset for dataset in troubs if len(troubs[dataset])==4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e7c21ce-42ae-4398-8790-6dab0a87c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert categorical variables into numerical\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    " \n",
    "# le = LabelEncoder()\n",
    " \n",
    "# # Using .fit_transform function to fit label\n",
    "# # encoder and return encoded label\n",
    "\n",
    "# base_path = 'Datasets/'\n",
    "# need_to_convert = ['abalone19-5-', 'abalone19-5-fold']\n",
    "# for needed in need_to_convert[1:]:\n",
    "#     for datFile in glob.glob(base_path+needed+'/*.dat'):\n",
    "#         print(datFile)\n",
    "#         df = read_dot_dat_file(datFile)\n",
    "#         label = le.fit_transform(df['Sex,'])        \n",
    "#         df['Sex,'] = label\n",
    "#         # df.to_excel(base_path+needed+'/'+datFile.split('/')[-1].split('.')[0]+'.xlsx')\n",
    "#         break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "138cde2d-9c7d-4e61-b733-cb4894e8dc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = read_dot_dat_file(glob.glob(base_path+datasets[0]+'/*.dat')[0])\n",
    "# df.to_excel('temp.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ec1b654-281a-4d72-ba83-ddafec3c725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_excel('temp.xlsx').iloc[:, 0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d75001-10b2-4a26-ba28-7ac17700e79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yeast-1-2-8-9_vs_7-5-fold RUS\n",
    "# shuttle-c2-vs-c4-5-fold SMOTE\n",
    "# ecoli-0-1-3-7_vs_2-6-5-fold SMOTE\n",
    "# yeast4-5-fold Ada1\n",
    "# abalone19-5- Ada1\n",
    "# abalone19-5-fold Ada1\n",
    "# abalone19-5- AdaCost\n",
    "# abalone19-5-fold AdaCost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d7806-0691-4fc2-85b5-c7f6ece4bd29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9720cc5-1b75-4ceb-8194-ca849754cd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
