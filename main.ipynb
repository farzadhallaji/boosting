{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30c6ba7c-abf0-4879-8daf-64cf03f708e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda install -c conda-forge imbalanced-learn\n",
    "# ! pip install nose\n",
    "# ! pip install imbalanced-ensemble           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a97ceea7-d25a-4b9f-9da9-66198967610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def read_dot_dat_file(path):\n",
    "    datContent = [i.strip().split() for i in open(path).readlines()]\n",
    "    r = re.compile(\"@inputs.*\")\n",
    "    _at_data = datContent.index(['@data'])\n",
    "    assert datContent[0][0] == '@relation'\n",
    "    assert datContent[_at_data-1][0] == '@outputs'\n",
    "    assert datContent[_at_data-2][0] == '@inputs'\n",
    "    print(datContent[_at_data-3][2:])\n",
    "    assert len(datContent[_at_data-3][2:]) == 2   # Two Class\n",
    "\n",
    "    col_names = datContent[_at_data-2][1:]\n",
    "    col_names.append(datContent[_at_data-1][1])\n",
    "    \n",
    "    df = pd.read_csv(path, skiprows=_at_data+1, names=col_names, sep=r', ', engine='python')\n",
    "    # df = pd.read_csv(path, skiprows=_at_data+1, names=col_names, sep=\", \", engine='python')\n",
    "\n",
    "    class1 = datContent[_at_data-3][2:][0].replace(\"{\",\"\").replace(\",\",\"\")\n",
    "    class2 = datContent[_at_data-3][2:][1].replace(\"}\",\"\").replace(\",\",\"\")\n",
    "\n",
    "    df['Class'] = df['Class'].replace({class1: 1, class2: -1})\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5780530b-2810-424b-8feb-0a847faf8ca1",
   "metadata": {},
   "source": [
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af89ab8a-ec78-4dbd-a01e-3d02ce85b6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd \n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# base_path = 'Datasets/'\n",
    "# need_to_convert = ['ecoli-0_vs_1-5-fold','haberman-5-fold','page-blocks0-5-fold','pima-5-fold','vehicle2-5-fold']\n",
    "\n",
    "# for needed in need_to_convert:\n",
    "#     for datFile in os.listdir(base_path+needed):\n",
    "#         print(base_path+needed+'/'+datFile)\n",
    "#         df = read_dot_dat_file(base_path+needed+'/'+datFile)\n",
    "#         df.to_excel(base_path+needed+'/'+datFile.split('.')[0]+'.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c891321f-9db4-46cb-87e0-3d1689692e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import random  \n",
    "import pandas as pd \n",
    "import random\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc,f1_score,matthews_corrcoef\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "\n",
    "\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from imbalanced_ensemble.ensemble import SMOTEBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f6a92d6-54cc-46a5-929a-3918bc55dabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Datasets/yeast-1_vs_7-5-fold/yeast-1_vs_7-5-1tra.xlsx', 'Datasets/yeast-1_vs_7-5-fold/yeast-1_vs_7-5-2tra.xlsx', 'Datasets/yeast-1_vs_7-5-fold/yeast-1_vs_7-5-3tra.xlsx', 'Datasets/yeast-1_vs_7-5-fold/yeast-1_vs_7-5-4tra.xlsx', 'Datasets/yeast-1_vs_7-5-fold/yeast-1_vs_7-5-5tra.xlsx']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mcc': [0.19062941194383556,\n",
       "  0.028416430811269728,\n",
       "  0.10852713178294573,\n",
       "  0.4453928491860374,\n",
       "  0.15819888872077387],\n",
       " 'auc': [0.6201550387596899,\n",
       "  0.5193798449612403,\n",
       "  0.554263565891473,\n",
       "  0.7926356589147286,\n",
       "  0.6078431372549019],\n",
       " 'f1': [0.25,\n",
       "  0.1111111111111111,\n",
       "  0.16666666666666666,\n",
       "  0.4705882352941177,\n",
       "  0.2222222222222222],\n",
       " 'gmean': [0.5498414147691576,\n",
       "  0.3812464258315117,\n",
       "  0.3962029078465307,\n",
       "  0.7825618830323859,\n",
       "  0.5423261445466404],\n",
       " 'exe_time': [1.0054960250854492,\n",
       "  0.9937365055084229,\n",
       "  1.2587642669677734,\n",
       "  0.8357181549072266,\n",
       "  0.854461669921875]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = 'Datasets/'\n",
    "datasets  = os.listdir(base_path) \n",
    "classifiers = {\"RUS\": RUSBoostClassifier(random_state=0, algorithm='SAMME', estimator=DecisionTreeClassifier(max_depth=10)),\n",
    "              \"SMOTE\": SMOTEBoostClassifier(estimator=DecisionTreeClassifier(max_depth=10), n_estimators = 100, algorithm='SAMME', random_state=0)}\n",
    "\n",
    "\n",
    "def clasify(dataset, classifier):\n",
    "    mcc = []\n",
    "    f1 = []\n",
    "    auc_a = []\n",
    "    gmean = []\n",
    "    times = []\n",
    "    traFiles = sorted(glob.glob(base_path+dataset+'/*tra.xlsx'))\n",
    "    tstFiles = sorted(glob.glob(base_path+dataset+'/*tst.xlsx'))\n",
    "    print(traFiles)\n",
    "    for traPath, tstPath in zip(traFiles, tstFiles):\n",
    "        \n",
    "        df_train = pd.read_excel(traPath)\n",
    "        df_test = pd.read_excel(tstPath)\n",
    "\n",
    "        x_train= df_train.iloc[:, 0:-1]\n",
    "        y_train = df_train.iloc[:, -1]\n",
    "        x_test= df_test.iloc[:, 0:-1]\n",
    "        y_test = df_test.iloc[:, -1]\n",
    "        \n",
    "        st = time.time()\n",
    "        clf = classifiers[classifier]\n",
    "\n",
    "        clf.fit(x_train, y_train)\n",
    "\n",
    "        y_pred = clf.predict(x_test) \n",
    "        et = time.time()\n",
    "        \n",
    "        # compute error\n",
    "        mcc.append(matthews_corrcoef(y_test, y_pred))\n",
    "        #--------------------------------\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "        auc_a.append(auc(fpr, tpr))\n",
    "        #--------------------------------\n",
    "        f1.append(f1_score(y_test, y_pred))\n",
    "        #--------------------------------\n",
    "        gmean.append(geometric_mean_score(y_test, y_pred, labels=[1, -1]))\n",
    "        \n",
    "        #time of train and test\n",
    "        times.append(et - st)\n",
    "        \n",
    "    return {\"mcc\": mcc, \"auc\": auc_a, \"f1\": f1, \"gmean\": gmean, \"exe_time\": times}\n",
    "\n",
    "clasify(datasets[0], 'SMOTE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
